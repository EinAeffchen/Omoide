NO_ALBUMENTATIONS_UPDATE=1

# Domain on which the platform will be hosted
DOMAIN=http://localhost:${PORT}

# Model to be used for search and media similarity
# options:
# 1. xlm-roberta-large-ViT-H-14 -> Cross language large model
# 2. xlm-roberta-base-ViT-B-32 -> Cross language small model 
# Below models are not tested yet and might need adjustments on the database 
# vector sizes
# 3. ViT-L-14 -> english only large model
# 4. laion2b_s32b_b82k -> english only large model
# 5. ViT-B-32 -> english only base model
# 6. convnext_base_w -> english only convolution base model
CLIP_MODEL=xlm-roberta-base-ViT-B-32

# Disables any edit/create/delete endpoints and hides related
# functions in the frontend
READ_ONLY=false
# enables detection and parsing of people/faces
ENABLE_PEOPLE=true
# enables automatic background scans for new files
AUTO_SCAN=true
# How often to scan for files in minutes
AUTO_SCAN_TIMEFRAME=15 
# If autoscan should also automatically cluster people - not recommended
AUTO_CLUSTER=false
# number of faces to batch at once: default: 10000, reduce in case of sudden container crash while clustering
CLUSTER_BATCH_SIZE=10000

# automatically detected exif rotation data and fixes the image on your drive
AUTO_ROTATE=true

VENV=./smol/.venv

# AI search parameters
MIN_SEARCH_DIST=0.68 # bigger = stricter search results
MIN_SIMILARITY_DIST=1.2 # bigger= stricter similarity measure

# AI processing parameters
# limit frames to process per video if not scenes are detected
MAX_FRAMES_PER_VIDEO=30 
FACE_RECOGNITION_MIN_CONFIDENCE=0.75 
FACE_MATCH_COSINE_THRESHOLD=0.55
# Minimum face size to detect in original image
FACE_RECOGNITION_MIN_FACE_PIXELS=3600
# Minimum number of faces to create a person from cluster
PERSON_MIN_FACE_COUNT=2

# settings for duplicate cleanup
# these settings apply for the duplicate detection on the duplicates page and are not applied
# automatically in a background process. If you choose the `nothing` option
# you can manually sort through the duplicates on the duplicates page.

# false = does not automatically perform actions on duplicates
# remove = remove duplicates from the database, this causes them to reappear on scan
# blacklist = remove duplicates from the database and blocks them from being reindexed
# delete = remove duplicates from the database and from your filesystem, this cannot be undone!
DUPLICATE_AUTO_HANDLING=false

# which file to keep from the duplicates
# biggest = file with biggst filesize
# smallest = file with smalles filesize
# highest_res = file with highest resolution
# lowest_res = file with lowest resolution
# oldest = file with oldest created_at date
# newest = file with newest created_at date
DUPLICATE_AUTO_KEEP_RULE=oldest
